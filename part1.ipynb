{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2bed1a9-dc5d-4a45-a295-69ca5c1166f2",
   "metadata": {},
   "source": [
    "# Part1: STAC,Geopandas, Xarray, Dask, Holoviz\n",
    "\n",
    "This notebook will showcase foundational open-source Python libraries in the Pangeo stack of tools, working up from small data to datasets that excede local memory.\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "- discover data with [STAC](https://stacspec.org) APIs\n",
    "- perform basic geospatial vector operations with [Geopandas](https://geopandas.org)\n",
    "- perform basic geospatial raster operations with [Xarray](http://xarray.pydata.org/en/stable/)/[Rioxarray](https://corteva.github.io/rioxarray/stable/)\n",
    "- single-machine scaling with [Dask](https://dask.org)\n",
    "- interactive browser visualizations with [Holoviz](https://holoviz.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf65a7-a168-43f3-9ced-42d9b15a6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC API search\n",
    "import pystac_client\n",
    "\n",
    "# Vector utilities\n",
    "import geopandas as gpd\n",
    "\n",
    "# Raster utilities\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "# Visualization\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "\n",
    "# Other misc tools\n",
    "import json\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05cb7dd-21b4-47aa-83cf-e2651a16be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's good practice to keep track of library versions\n",
    "print(f'pystac_client={pystac_client.__version__}')\n",
    "print(f'geopandas={gpd.__version__}')\n",
    "print(f'xarray={xr.__version__}')\n",
    "print(f'rioxarray={rioxarray.__version__}')\n",
    "print(f'hvplot={hvplot.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f336f0b-4544-449b-9c97-c22ad52ba559",
   "metadata": {},
   "source": [
    "## Vector data \n",
    "\n",
    "Geospatial vector data consists basic geometries (Points, Lines, Polygons) with coordinate reference system information (CRS). If you're new to vector data, check out this Software Carpentry [lesson](https://carpentries-incubator.github.io/geospatial-python/).\n",
    "\n",
    "[Pandas](https://pandas.pydata.org) is a core scientific Python library to work with \"Panel Data\" (PanDas). Basically if you have a spreadsheet or database you should be using Pandas! Pandas has many input/output (I/O) functions, and two core data structures - the \"Series\" and \"DataFrame\". \n",
    "\n",
    "[Geopandas](http://geopandas.org) extends Pandas to work efficently with collections of geographic Vector data - geometric shapes that are georeferenced to a position on Earth's surface. Geopandas data objects are, you might have guessed, called \"GeoSeries\" and \"GeoDataFrame\".\n",
    "\n",
    "There are *many* vector formats for geospatial data. A very common one is [GeoJSON](https://gdal.org/drivers/vector/geojson.html), which can be easily represented as a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff923c68-74d3-4ab9-b4d1-6ebf3ec4905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barreal, Argentina location in GeoJSON\n",
    "# from https://geojson.io\n",
    "\n",
    "area_of_interest = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Point\",\n",
    "        \"coordinates\": [\n",
    "          -69.466552734375,\n",
    "          -31.62532121329918\n",
    "          \n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "with open('point.geojson', 'w') as f:\n",
    "   json.dump(area_of_interest, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c13e3-37b3-4970-9fff-042777388509",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = gpd.read_file('point.geojson')\n",
    "gf['id'] = 'barreal'\n",
    "gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0767b-e95d-49a0-b04d-2142f5de7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geopandas facilitates geospatial operations such as reprojection\n",
    "# https://epsg.io/32719\n",
    "gf_utm = gf.to_crs('EPSG:32719')\n",
    "gf_utm.buffer(100) \n",
    "# Why reproject? What are the units here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f42be-caaf-41de-9267-188401769258",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- We created a simple GeoPandas Dataframe for a POINT geometry\n",
    "- Geopandas was used to reproject coordinate reference system and add a buffer to form a POLYGON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8065c-fdd8-4182-8d06-4e546689a5f6",
   "metadata": {},
   "source": [
    "## Search for data\n",
    "\n",
    "[SpatioTemporal Asset Catalogs (STAC)](https://stacspec.org) are a standard among imagery providers to simplify and unify search capabilities. Metadata is in JSON format and definited by a community-built standard [core specification](https://github.com/radiantearth/stac-spec) with optional [extensions](https://stac-extensions.github.io).\n",
    "\n",
    "[pystac_client](https://github.com/stac-utils/pystac-client) is a Python client for working with STAC Catalogs and APIs. It uses [PySTAC](https://pystac.readthedocs.io) behind the scenes to navigate STAC metadata.\n",
    "\n",
    "There are several public STAC API endpoints, which you can find on the [STAC Index Website](https://stacindex.org/catalogs?access=public&type=api), a few are listed below:\n",
    "\n",
    "| provider | endpoint | datacenters |\n",
    "| - | - | - | \n",
    "| [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/) | https://planetarycomputer.microsoft.com/api/stac/v1 | Azure West Europe |\n",
    "| [Element84 Earthsearch](https://www.element84.com/earth-search/) | https://earth-search.aws.element84.com/v0 | AWS multiple regions | \n",
    "| [NASA CMR STAC Cloud Proxy](https://github.com/nasa/cmr-stac) | https://cmr.earthdata.nasa.gov/cloudstac | AWS us-west-2 | \n",
    "\n",
    "For high-performance and cost-effective analysis, always keep in mind where data is located! For this workshop we are running on servers in Microsoft Azure’s `West Europe` region, so we'll use mostly datasets hosted in that region by the [Planetary Computer Initiative](https://planetarycomputer.microsoft.com). We run computations where the data is stored, and bring small subsets or visualizations back for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7593f2-8800-41f8-88e1-1b2d2eab3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "\n",
    "# Connect to a STAC API\n",
    "\n",
    "# See documentation of all datasets at https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/\n",
    "# See also data catalog website: https://planetarycomputer.microsoft.com/catalog\n",
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "# Use the 'geometry' information from GeoJSON\n",
    "search = catalog.search(collections=[\"nasadem\"], \n",
    "                        intersects=area_of_interest['features'][0]['geometry'], \n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71991b-6a65-4f90-bb24-32290587bc97",
   "metadata": {},
   "source": [
    "STAC ItemCollections can be represented as *vector* data in GeoJSON format !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4f2fe-e348-402d-bd8c-1b6ba8f48fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenient way to display results is as a Geopandas GeoDataFrame\n",
    "gf = gpd.GeoDataFrame.from_features(search.get_all_items_as_dict())\n",
    "\n",
    "# BUG: 'ids' droppped https://github.com/geopandas/geopandas/pull/2003\n",
    "gf['id'] = [item.id for item in search.get_all_items()]\n",
    "gf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa612d-6647-44b0-8f30-35d08f09f948",
   "metadata": {},
   "source": [
    "**Aside:** Above we have a simple work-around for a geopandas bug. If open-source libraries are missing some functionality you can help! In fact, the success of open source software relies on community contributions and volunteer efforts. Remember, you are not just a *user* of these tools, but a *supporter* of these tools. Check out the excellent Xarray contributing guide for ideas of how to get started contributing http://xarray.pydata.org/en/stable/contributing.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf0d14-ba95-4356-a845-a55626254468",
   "metadata": {},
   "source": [
    "### Visualization of geospatial data\n",
    "\n",
    "In this notebook we're going to illustrate the use of [hvplot](https://hvplot.holoviz.org/user_guide/Geographic_Data.html), which works very well to visualize both vector and raster data in a jupyter notebook. `hvplot` is used as an \"accessor\" on pandas and xarray objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f50980-b7ad-478f-b416-7ed288cf64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above we can use holoviews to plot this\n",
    "gf.hvplot.polygons(geo=True, tiles=True, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306bd18-63e6-45c5-b59a-4dfa05d460b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PySTAC to iterate through STAC Items\n",
    "for item in search.get_all_items():\n",
    "    print(item.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056435d-0426-4c71-b166-7f1eae86146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a single STAC Item, that can contain multiple STAC Assets:\n",
    "for asset_key, asset in item.assets.items():\n",
    "    print(f\"{asset_key:<10} - {asset.href}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac09b8-7b4e-41a1-b636-77fb81256c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cc185-60ee-4f42-81f1-d157eedc9599",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- We found elevation data covering our point via `pystac_client` microsoft planetary computer's STAC API\n",
    "- We converted the STAC search Item footprints into a Geopandas dataframe and vizualied them with hvplot.\n",
    "- We navigated the STAC metadata with PySTAC\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693be926-286c-409d-9f64-cc3670bdbba3",
   "metadata": {},
   "source": [
    "## Raster data\n",
    "\n",
    "We've seen that [Pandas](https://pandas.pydata.org/pandas-docs/stable/) and [Geopandas](http://geopandas.org) are excellent libraries for analyzing tabular \"labeled data\". [Xarray](http://xarray.pydata.org/en/stable/) is designed to make it easier to work with with _labeled multidimensional data_. By _multidimensional data_ (also often called _N-dimensional_), we mean data with many independent dimensions or axes. For example, we might represent Earth's surface temperature $T$ as a three dimensional variable\n",
    "\n",
    "$$ T(x, y, t) $$\n",
    "\n",
    "where $x$ and $y$ are spatial dimensions and and $t$ is time. By _labeled_, we mean data that has metadata associated with it describing the names and relationships between the variables. The cartoon below shows a \"data cube\" schematic dataset with temperature and preciptation sharing the same three dimensions, plus longitude and latitude as auxilliary coordinates.\n",
    "\n",
    "![xarray data model](https://github.com/pydata/xarray/raw/main/doc/_static/dataset-diagram.png)\n",
    "\n",
    "### Xarray data structures\n",
    "\n",
    "Like Pandas, xarray has two fundamental data structures:\n",
    "* a `DataArray`, which holds a single multi-dimensional variable and its coordinates\n",
    "* a `Dataset`, which holds multiple variables that potentially share the same coordinates\n",
    "\n",
    "#### DataArray\n",
    "\n",
    "A `DataArray` has four essential attributes:\n",
    "* `values`: a `numpy.ndarray` holding the array’s values\n",
    "* `dims`: dimension names for each axis (e.g., `('x', 'y', 'z')`)\n",
    "* `coords`: a dict-like container of arrays (coordinates) that label each point (e.g., 1-dimensional arrays of numbers, datetime objects or strings)\n",
    "* `attrs`: an `OrderedDict` to hold arbitrary metadata (attributes)\n",
    "\n",
    "#### DataSet\n",
    "\n",
    "A dataset is simply an object containing multiple DataArrays indexed by variable name\n",
    "\n",
    "\n",
    "### RioXarray Extension\n",
    "\n",
    "Note that Xarray is a generic nD array library and therefore does not handle geospatial-specific functionality on its own (like Coordinate Reference System (CRS) managment, reprojection, etc.). The [RioXarray extension](https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html) adds that functionality! RioXarray build on top of [rasterio](https://rasterio.readthedocs.io/en/latest/), which is built on top of [GDAL](https://gdal.org), so you can see it's FOSS4G all the way down! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f6a03-8fe0-4673-b424-dd712f2942c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we are passing a URL here rather than a local file path\n",
    "\n",
    "da = rioxarray.open_rasterio(asset.href, masked=True)\n",
    "da.name = item.id\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddff54-6ebe-459f-bdf8-ef0e32a77390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using rioxarray, we have regular xarray objects that are enhanced with a '.rio' accessor that adds functionality\n",
    "da.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8026311-398f-46bf-aefa-6639ab987b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection is trivial\n",
    "da_utm = da.rio.reproject(da.rio.estimate_utm_crs())\n",
    "print(da_utm.rio.crs)\n",
    "da_utm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34ca7d-229b-49ce-9a3b-224876397dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xarray has very convenient label-based indexing so you don't have to remember integer axis ordering\n",
    "\n",
    "# Here we get an East-West Profile through a particular latitude\n",
    "da.sel(band=1, y=-31.5, method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf59628-d6c9-4d17-8556-8193ae80efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use hvplot on xarray objects for interactive plots in the browser\n",
    "# Using bokeh toolbars, resolution is updated as you zoom in!\n",
    "\n",
    "da.hvplot.image(x='x',y='y',\n",
    "                geo=True, # convey that 'x' and 'y' correspond to 'longitude, latitude'\n",
    "                rasterize=True, # don't send entire array to browser, just rendered image\n",
    "                cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6bf01-d596-45f2-a401-eeaf9119e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the profile line we extracted\n",
    "da.sel(band=1, y=-31.5, method='nearest').hvplot.scatter(xlabel='longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21965533-c6cd-426b-b924-8c4d76ebc81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same profile line in UTM (both axes in meters)\n",
    "transform = pyproj.Proj(da_utm.rio.crs)\n",
    "easting, northing = transform(-70, -31.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb64dae-c7b9-4e38-89a1-e7252ff24dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_utm.sel(band=1, y=northing, method='nearest').hvplot.scatter(xlabel='easting (m)', data_aspect=1, frame_width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af321420-9c69-4b1d-830f-5c5f25885fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that if we run calculations on the entire DataArray, all data is read into local RAM and the result is presented to us:\n",
    "da.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c1405-18f2-4e1a-806d-ac32975c56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! \n",
    "\n",
    "# - Pick a different point in Argentina and make some plots of elevation\n",
    "# - Try some different plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5dae4-a22e-4ef5-afef-e3181feea094",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "\n",
    "[Dask](https://docs.dask.org/en/latest/) is a flexible parallel computing library for analytic computing. Dask provides dynamic parallel task scheduling and high-level big-data collections like `dask.array` and `dask.dataframe`. \n",
    "\n",
    "Dask is tightly integrated with Xarray, such that parallel computations can happen with very little change to your code. Often, you only need to specify dask \"chunks\", which describe how an array is split up over many sub-arrays.\n",
    "\n",
    "![Dask Arrays](http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg)\n",
    "_source: [Dask Array Documentation](http://dask.pydata.org/en/latest/array-overview.html)_\n",
    "\n",
    "There are [several ways to specify chunks](https://docs.dask.org/en/latest/array-chunks.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddfcec-0dbc-470d-8adc-811e4837e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although the array size is small, we can use our elevation example to understand how dask integrates with xarray\n",
    "print(type(da.data))\n",
    "da = da.chunk(dict(x=2048,y=2048))\n",
    "print(type(da.data))\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d06720-8e2f-47c5-83f4-bd68a8e91852",
   "metadata": {},
   "source": [
    "Note that xarray now provides us with a nice HTML representation of the underlying dask array and how it is divided into chunks. Ofen the best performance is obtained when chunks match data storage. This elevation data is stored as a Cloud Optimized Geotiff or ['COG'](https://www.cogeo.org), which typically are stored as 512x512 tiles. If you really want to dig into performance, have a look through [these notebooks](https://github.com/pangeo-data/cog-best-practices), but it's usually okay to pick chunks that are a multiple of the data tile size (512*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604f0e3-d45d-46a0-aa6a-228d0e2038e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8ed127-3588-4bdd-a01f-a6a206b0914b",
   "metadata": {},
   "source": [
    "### Dask clusters\n",
    "\n",
    "Dask needs a collection of computing resources in order to perform parallel computations. Dask Clusters have different names corresponding to different computing environments (for example, [LocalCluster](https://distributed.dask.org/en/latest/local-cluster.html) for a single machine, [PBSCluster](http://jobqueue.dask.org/) for your HPC, or [Kubernetes Cluster](http://kubernetes.dask.org/) for machines distributed on the Cloud). Each cluster has a certain number of computing resources called 'Workers', that each get allocated CPU and RAM. The dask scheduling system maps jobs to each worker on a cluster for you, so the syntax is mostly the same once you initialize a cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbe940-f7b9-4662-8daa-87b4f5ead510",
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's start simple with a LocalCluster that makes use of all the cores and RAM we have on a single machine\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "# explicitly connect to the cluster we just created\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b5e71-2bc3-4dd7-8f7e-07cdbc4a09a2",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "There are lots of great tutorials out there to continue to explore.\n",
    "\n",
    "- The Official Documentation for each Library\n",
    "- [Pangeo Gallery Tutorial](https://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/)\n",
    "- [Earthlab Data Science Course](https://www.earthdatascience.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d8919-7a7c-49dc-88b3-f48543903fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
