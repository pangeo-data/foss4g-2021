{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2bed1a9-dc5d-4a45-a295-69ca5c1166f2",
   "metadata": {},
   "source": [
    "# STAC, Geopandas, Xarray, Dask, Holoviz\n",
    "\n",
    "This notebook will showcase foundational open-source Python libraries in the Pangeo stack of tools, working up from small data to datasets that excede local memory. We'll be going over this notebook at [FOSS4G 2021 Buenos Aires Workshop](https://callforpapers.2021.foss4g.org/foss4g-2021-workshop/talk/NMFGKD/). Plan to spend at least 30 minutes to 1 hour going over this notebook!\n",
    "\n",
    "\n",
    "## Learning objectives:\n",
    "\n",
    "- discover data with [STAC](https://stacspec.org) APIs\n",
    "- perform basic geospatial vector operations with [Geopandas](https://geopandas.org)\n",
    "- perform basic geospatial raster operations with [Xarray](http://xarray.pydata.org/en/stable/)/[Rioxarray](https://corteva.github.io/rioxarray/stable/)\n",
    "- single-machine scaling with [Dask](https://dask.org)\n",
    "- interactive browser visualizations with [Holoviz](https://holoviz.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf65a7-a168-43f3-9ced-42d9b15a6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC API search\n",
    "import pystac_client\n",
    "\n",
    "# Vector utilities\n",
    "import geopandas as gpd\n",
    "\n",
    "# Raster utilities\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import stackstac\n",
    "\n",
    "# Visualization\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "\n",
    "# Other tools\n",
    "import json\n",
    "import pyproj\n",
    "import planetary_computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05cb7dd-21b4-47aa-83cf-e2651a16be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's good practice to keep track of library versions\n",
    "print(f'pystac_client={pystac_client.__version__}')\n",
    "print(f'geopandas={gpd.__version__}')\n",
    "print(f'xarray={xr.__version__}')\n",
    "print(f'rioxarray={rioxarray.__version__}')\n",
    "print(f'hvplot={hvplot.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f336f0b-4544-449b-9c97-c22ad52ba559",
   "metadata": {},
   "source": [
    "## Vector data \n",
    "\n",
    "Geospatial vector data consists basic geometries (Points, Lines, Polygons) with coordinate reference system information (CRS). If you're new to vector data, check out this Software Carpentry [lesson](https://carpentries-incubator.github.io/geospatial-python/).\n",
    "\n",
    "[Pandas](https://pandas.pydata.org) is a core scientific Python library to work with \"Panel Data\" (PanDas). Basically if you have a spreadsheet or database you should be using Pandas! Pandas has many input/output (I/O) functions, and two core data structures - the \"Series\" and \"DataFrame\". \n",
    "\n",
    "[Geopandas](http://geopandas.org) extends Pandas to work efficently with collections of geographic Vector data - geometric shapes that are georeferenced to a position on Earth's surface. Geopandas data objects are, you might have guessed, called \"GeoSeries\" and \"GeoDataFrame\".\n",
    "\n",
    "There are *many* vector formats for geospatial data. A very common one is [GeoJSON](https://gdal.org/drivers/vector/geojson.html), which can be easily represented as a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff923c68-74d3-4ab9-b4d1-6ebf3ec4905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barreal, Argentina location in GeoJSON\n",
    "# from https://geojson.io\n",
    "\n",
    "area_of_interest = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Point\",\n",
    "        \"coordinates\": [\n",
    "          -69.466552734375,\n",
    "          -31.62532121329918\n",
    "          \n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "with open('point.geojson', 'w') as f:\n",
    "   json.dump(area_of_interest, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c13e3-37b3-4970-9fff-042777388509",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_aoi = gpd.read_file('point.geojson')\n",
    "gf_aoi['id'] = 'barreal'\n",
    "gf_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0767b-e95d-49a0-b04d-2142f5de7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geopandas facilitates geospatial operations such as reprojection\n",
    "# https://epsg.io/32719\n",
    "gf_aoi_utm = gf_aoi.to_crs('EPSG:32719')\n",
    "gf_aoi_utm.buffer(100) \n",
    "# Why reproject? What are the units here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f42be-caaf-41de-9267-188401769258",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- We created a simple GeoPandas Dataframe for a POINT geometry\n",
    "- Geopandas was used to reproject coordinate reference system and add a buffer to form a POLYGON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8065c-fdd8-4182-8d06-4e546689a5f6",
   "metadata": {},
   "source": [
    "## Search for data\n",
    "\n",
    "[SpatioTemporal Asset Catalogs (STAC)](https://stacspec.org) are a standard among imagery providers to simplify and unify search capabilities. Metadata is in JSON format and definited by a community-built standard [core specification](https://github.com/radiantearth/stac-spec) with optional [extensions](https://stac-extensions.github.io).\n",
    "\n",
    "[pystac_client](https://github.com/stac-utils/pystac-client) is a Python client for working with STAC Catalogs and APIs. It uses [PySTAC](https://pystac.readthedocs.io) behind the scenes to navigate STAC metadata.\n",
    "\n",
    "There are several public STAC API endpoints, which you can find on the [STAC Index Website](https://stacindex.org/catalogs?access=public&type=api), a few are listed below:\n",
    "\n",
    "| provider | endpoint | datacenters |\n",
    "| - | - | - | \n",
    "| [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/) | https://planetarycomputer.microsoft.com/api/stac/v1 | Azure West Europe |\n",
    "| [Element84 Earthsearch](https://www.element84.com/earth-search/) | https://earth-search.aws.element84.com/v0 | AWS multiple regions | \n",
    "| [NASA CMR STAC Cloud Proxy](https://github.com/nasa/cmr-stac) | https://cmr.earthdata.nasa.gov/cloudstac | AWS us-west-2 | \n",
    "\n",
    "For high-performance and cost-effective analysis, always keep in mind where data is located! For this workshop we are running on servers in Microsoft Azureâ€™s `West Europe` region, so we'll use mostly datasets hosted in that region by the [Planetary Computer Initiative](https://planetarycomputer.microsoft.com). We run computations where the data is stored, and bring small subsets or visualizations back for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7593f2-8800-41f8-88e1-1b2d2eab3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See documentation of all datasets at https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac/\n",
    "# See also data catalog website: https://planetarycomputer.microsoft.com/catalog\n",
    "catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "collections = catalog.get_children()\n",
    "for collection in collections:\n",
    "    print(f\"{collection.id} - {collection.title}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e4ec6-1963-4d25-8a13-0ee2401495af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'geometry' information from our GeoJSON\n",
    "search = catalog.search(collections=[\"nasadem\"], \n",
    "                        intersects=area_of_interest['features'][0]['geometry'], \n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71991b-6a65-4f90-bb24-32290587bc97",
   "metadata": {},
   "source": [
    "STAC ItemCollections can be represented as *vector* data in GeoJSON format !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4f2fe-e348-402d-bd8c-1b6ba8f48fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenient way to display results is as a Geopandas GeoDataFrame\n",
    "gf = gpd.GeoDataFrame.from_features(search.get_all_items_as_dict())\n",
    "\n",
    "# BUG: 'ids' droppped https://github.com/geopandas/geopandas/pull/2003\n",
    "gf['id'] = [item.id for item in search.get_all_items()]\n",
    "gf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa612d-6647-44b0-8f30-35d08f09f948",
   "metadata": {},
   "source": [
    "**Aside:** Above we have a simple work-around for a geopandas bug. If open-source libraries are missing some functionality you can help! In fact, the success of open source software relies on community contributions and volunteer efforts. Remember, you are not just a *user* of these tools, but a *supporter* of these tools. Check out the excellent Xarray contributing guide for ideas of how to get started contributing http://xarray.pydata.org/en/stable/contributing.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf0d14-ba95-4356-a845-a55626254468",
   "metadata": {},
   "source": [
    "### Visualization of geospatial data\n",
    "\n",
    "In this notebook we're going to illustrate the use of [hvplot](https://hvplot.holoviz.org/user_guide/Geographic_Data.html), which works very well to visualize both vector and raster data in a jupyter notebook. `hvplot` is used as an \"accessor\" on pandas and xarray objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f50980-b7ad-478f-b416-7ed288cf64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above we can use holoviews to plot this\n",
    "dem_footprint = gf.hvplot.polygons(geo=True, alpha=0.2)\n",
    "dem_footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306bd18-63e6-45c5-b59a-4dfa05d460b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PySTAC to iterate through STAC Items\n",
    "for item in search.get_all_items():\n",
    "    print(item.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901c616-5b39-4617-b7f7-1f3487c6d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before doing any analysis with new datasets, take a step back and acquaint yourself with the details!\n",
    "collection = item.get_collection()\n",
    "collection.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d644484-dbe3-41c8-a812-ebf46ecbca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.to_dict() for p in collection.providers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119b4b3-8894-4bc0-bb9d-76777259e3e7",
   "metadata": {},
   "source": [
    "STAC metadata keeps track of data provenance, so you should always be able to get back to the original data producer or 'archive of record'. In the case of NASADEM it's NASA's LPDAAC data center https://lpdaac.usgs.gov/products/nasadem_hgtv001/. On that page you'll find important information that might not be present in STAC metadata, such as the vertical reference or 'datum' of elevation values, which is \"EGM96\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056435d-0426-4c71-b166-7f1eae86146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a single STAC Item, that can contain multiple STAC Assets:\n",
    "for asset_key, asset in item.assets.items():\n",
    "    print(f\"{asset_key:<10} - {asset.href}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac09b8-7b4e-41a1-b636-77fb81256c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single Items it's sometimes easier to convert STAC JSON to a python dictionary\n",
    "item.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cc185-60ee-4f42-81f1-d157eedc9599",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- We found elevation data covering our point via `pystac_client` microsoft planetary computer's STAC API\n",
    "- We converted the STAC search Item footprint into a Geopandas dataframe and vizualized it with hvplot.\n",
    "- We navigated the STAC metadata with PySTAC and as a python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693be926-286c-409d-9f64-cc3670bdbba3",
   "metadata": {},
   "source": [
    "## Raster data\n",
    "\n",
    "We've seen that [Pandas](https://pandas.pydata.org/pandas-docs/stable/) and [Geopandas](http://geopandas.org) are excellent libraries for analyzing tabular \"labeled data\". [Xarray](http://xarray.pydata.org/en/stable/) is designed to make it easier to work with with _labeled multidimensional data_. By _multidimensional data_ (also often called _N-dimensional_), we mean data with many independent dimensions or axes. For example, we might represent Earth's surface temperature $T$ as a three dimensional variable\n",
    "\n",
    "$$ T(x, y, t) $$\n",
    "\n",
    "where $x$ and $y$ are spatial dimensions and and $t$ is time. By _labeled_, we mean data that has metadata associated with it describing the names and relationships between the variables. The cartoon below shows a \"data cube\" schematic dataset with temperature and preciptation sharing the same three dimensions, plus longitude and latitude as auxilliary coordinates.\n",
    "\n",
    "![xarray data model](https://github.com/pydata/xarray/raw/main/doc/_static/dataset-diagram.png)\n",
    "\n",
    "### Xarray data structures\n",
    "\n",
    "Like Pandas, xarray has two fundamental data structures:\n",
    "* a `DataArray`, which holds a single multi-dimensional variable and its coordinates\n",
    "* a `Dataset`, which holds multiple variables that potentially share the same coordinates\n",
    "\n",
    "#### DataArray\n",
    "\n",
    "A `DataArray` has four essential attributes:\n",
    "* `values`: a `numpy.ndarray` holding the arrayâ€™s values\n",
    "* `dims`: dimension names for each axis (e.g., `('x', 'y', 'z')`)\n",
    "* `coords`: a dict-like container of arrays (coordinates) that label each point (e.g., 1-dimensional arrays of numbers, datetime objects or strings)\n",
    "* `attrs`: an `OrderedDict` to hold arbitrary metadata (attributes)\n",
    "\n",
    "#### DataSet\n",
    "\n",
    "A dataset is simply an object containing multiple DataArrays indexed by variable name\n",
    "\n",
    "\n",
    "### RioXarray Extension\n",
    "\n",
    "Note that Xarray is a generic nD array library and therefore does not handle geospatial-specific functionality on its own (like Coordinate Reference System (CRS) managment, reprojection, etc.). The [RioXarray extension](https://corteva.github.io/rioxarray/stable/getting_started/getting_started.html) adds that functionality! RioXarray build on top of [rasterio](https://rasterio.readthedocs.io/en/latest/), which is built on top of [GDAL](https://gdal.org), so you can see it's FOSS4G all the way down! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f6a03-8fe0-4673-b424-dd712f2942c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: we are passing a URL here rather than a local file path\n",
    "da = rioxarray.open_rasterio(asset.href, masked=True)\n",
    "da.name = item.id\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddff54-6ebe-459f-bdf8-ef0e32a77390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using rioxarray, we have regular xarray objects that are enhanced with a '.rio' accessor that adds functionality\n",
    "da.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8026311-398f-46bf-aefa-6639ab987b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection syntax is similar to geopandas\n",
    "da_utm = da.rio.reproject(da.rio.estimate_utm_crs())\n",
    "print(da_utm.rio.crs)\n",
    "da_utm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34ca7d-229b-49ce-9a3b-224876397dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xarray has very convenient label-based indexing so you don't have to remember integer axis ordering\n",
    "\n",
    "# Here we get an East-West Profile through a particular latitude\n",
    "da.sel(band=1, y=-31.5, method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf59628-d6c9-4d17-8556-8193ae80efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use hvplot on xarray objects for interactive plots in the browser\n",
    "# Using bokeh toolbars, resolution is updated as you zoom in!\n",
    "\n",
    "da.hvplot.image(x='x',y='y',\n",
    "                geo=True, # convey that 'x' and 'y' correspond to 'longitude, latitude'\n",
    "                rasterize=True, # don't send entire array to browser, just rendered image\n",
    "                cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6bf01-d596-45f2-a401-eeaf9119e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the profile line we extracted\n",
    "da.sel(band=1, y=-31.5, method='nearest').hvplot.scatter(xlabel='longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21965533-c6cd-426b-b924-8c4d76ebc81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same profile line in UTM (both axes in meters)\n",
    "transform = pyproj.Proj(da_utm.rio.crs)\n",
    "easting, northing = transform(-70, -31.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb64dae-c7b9-4e38-89a1-e7252ff24dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_utm.sel(band=1, y=northing, method='nearest').hvplot.scatter(xlabel='easting (m)', data_aspect=1, frame_width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af321420-9c69-4b1d-830f-5c5f25885fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that if we run calculations on the entire DataArray, all data is read into local RAM and the result is presented to us:\n",
    "da.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c1405-18f2-4e1a-806d-ac32975c56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! \n",
    "\n",
    "# - Pick a different point in Argentina and make some plots of elevation\n",
    "# - Try some different plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5dae4-a22e-4ef5-afef-e3181feea094",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "\n",
    "[Dask](https://docs.dask.org/en/latest/) is a flexible parallel computing library for analytic computing. Dask provides dynamic parallel task scheduling and high-level big-data collections like `dask.array` and `dask.dataframe`. \n",
    "\n",
    "Dask is tightly integrated with Xarray, such that parallel computations can happen with very little change to your code. Often, you only need to specify dask \"chunks\", which describe how an array is split up over many sub-arrays.\n",
    "\n",
    "![Dask Arrays](http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg)\n",
    "_source: [Dask Array Documentation](http://dask.pydata.org/en/latest/array-overview.html)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddfcec-0dbc-470d-8adc-811e4837e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although the array size is small, we can use our elevation example to understand how dask integrates with xarray\n",
    "print(type(da.data))\n",
    "da = da.chunk(dict(x=2048,y=2048))\n",
    "print(type(da.data))\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d06720-8e2f-47c5-83f4-bd68a8e91852",
   "metadata": {},
   "source": [
    "Note that xarray now provides us with a nice HTML representation of the underlying dask array and how it is divided into 4 chunks. Often the best performance is obtained when chunks match data storage. NASADEM is stored as a Cloud Optimized Geotiff or ['COG'](https://www.cogeo.org), which typically are stored as 512x512 tiles. If you really want to dig into performance, have a look through [these notebooks](https://github.com/pangeo-data/cog-best-practices), but it's usually okay to pick chunk sizes that are a multiple of the data tile size (512*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604f0e3-d45d-46a0-aa6a-228d0e2038e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's again just look at the mean of all the pixels\n",
    "da.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70324688-789c-4173-9ab8-399c6851b11c",
   "metadata": {},
   "source": [
    "**What happened?**\n",
    "\n",
    "now that our DataArray has dask arrays, only the 'task graph' of computations is created, and those computations are not triggered until explicitly asked for or visualized. This is known as 'lazy computation'. You can trigger the computation with a `.compute()` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c5881-a700-432f-a004-4fe3466a092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab4199-0388-4e5e-9f9f-6bf15fcbff91",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- We converted our array into a dask-array and let dask manage a parallel computation for us on our raster divided into 4 chunks\n",
    "- By default, dask uses its [single-machine multi-threaded scheduler](https://docs.dask.org/en/latest/setup/single-machine.html), which distributes work across multiple local CPU threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ed127-3588-4bdd-a01f-a6a206b0914b",
   "metadata": {},
   "source": [
    "### Dask clusters\n",
    "\n",
    "Dask needs a collection of computing resources in order to perform parallel computations. Dask Clusters have different names corresponding to different computing environments (for example, [LocalCluster](https://distributed.dask.org/en/latest/local-cluster.html) for a single machine, [PBSCluster](http://jobqueue.dask.org/) for your HPC, or [Kubernetes Cluster](http://kubernetes.dask.org/) for machines distributed on the Cloud). Each cluster has a certain number of computing resources called 'Workers', that each get allocated CPU and RAM. The dask scheduling system maps jobs to each worker on a cluster for you, so the syntax is mostly the same once you initialize a cluster!\n",
    "\n",
    "The key advantage of explicitly creating a cluster is that it gives you visual dashboard [diagnostics](https://docs.dask.org/en/latest/diagnostics-distributed.html) (either on a standalone 'Dashboard' webpage or visualized in JupyterLab with the [Dask Labextension](https://github.com/dask/dask-labextension) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbe940-f7b9-4662-8daa-87b4f5ead510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start simple with a LocalCluster that makes use of all the cores and RAM we have on a single machine\n",
    "# NOTE: by default this cluster will use multiple 'processes' instead of threads by default\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "\n",
    "# explicitly connect to the cluster we just created\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d660a46-8567-471b-8a28-8c13541ba227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the standard deviation and watch the diagnotic dashboard 'task stream' \n",
    "da.std().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c98a999-b011-42ec-a8bf-384a3dec7481",
   "metadata": {},
   "source": [
    "## Scaling on a single machine\n",
    "\n",
    "Let's use our LocalCluster to do some analysis on a dataset that would otherwise excede our 16GB of memory. Dask will ensure that we don't hit out of memory errors as it crunches through our computation on multiple threads. For illustration we'll use a STAC of Landsat8 scenes... p.s. Landsat9 Successfully [launched yesterday 9/27/2021](https://www.nasa.gov/press-release/nasa-launches-new-mission-to-monitor-earth-s-landscapes)!! \n",
    "\n",
    "There are a lot of Landsat archives out there with subtle differences, note this dataset is coming from the US Geological Survey and the original archive is hosted on Amazon Web Services in a requester-pays bucket\n",
    "https://www.usgs.gov/core-science-systems/nli/landsat/landsat-commercial-cloud-data-access?qt-science_support_page_related_con=1#qt-science_support_page_related_con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb56cd-84f5-4ff4-a631-4b8ff0f2f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the full documentation at https://www.usgs.gov/core-science-systems/nli/landsat/landsat-collection-2\n",
    "\n",
    "search = catalog.search(collections=[\"landsat-8-c2-l2\"], \n",
    "                        intersects=area_of_interest['features'][0]['geometry'], \n",
    "                        datetime=['2015-01-01T00:00:00Z', None],\n",
    "                        # Only keep 'Tier1' data 'suitable for timeseries analysis'\n",
    "                        query=['landsat:collection_category=T1', 'landsat:wrs_path=232'], \n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52c54c-696b-451a-9401-0122b70cd6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenient way to display results is as a Geopandas GeoDataFrame\n",
    "gf = gpd.GeoDataFrame.from_features(search.get_all_items_as_dict())\n",
    "print(len(gf))\n",
    "# BUG: 'ids' droppped https://github.com/geopandas/geopandas/pull/2003\n",
    "gf['id'] = [item.id for item in search.get_all_items()]\n",
    "gf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5968beb6-e04e-46f9-8b9d-829da99dc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composing holoviz elements in single visualization (basemap tiles, polygons and point)\n",
    "polygons = gf.hvplot.polygons(geo=True, \n",
    "                   tiles='EsriTerrain', \n",
    "                   hover_cols=['landsat:wrs_path','landsat:wrs_row'],\n",
    "                   alpha=0.2)\n",
    "\n",
    "polygons * dem_footprint * gf_aoi.hvplot.points(geo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da59bfa-6f5f-4d39-a58f-2505846591a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read this data, we need to sign urls as described here \n",
    "# https://planetarycomputer.microsoft.com/docs/concepts/sas/\n",
    "items = search.get_all_items()\n",
    "\n",
    "signed_items = []\n",
    "for item in items:\n",
    "    item = item.clone()\n",
    "    # Fix metadata issue https://github.com/stactools-packages/landsat/issues/12\n",
    "    item.properties['proj:epsg']=32619\n",
    "    item.clear_links()\n",
    "    signed_items.append(planetary_computer.sign(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7991ae-1765-43c0-8fc0-8700691e544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can open a single asset as before\n",
    "item = signed_items[0]\n",
    "da = rioxarray.open_rasterio(item.assets['SR_B4'].href, masked=True)\n",
    "da.sel(band=1).hvplot(x='x',y='y',rasterize=True)\n",
    "da.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0dbefd-5608-481f-8739-df31061e5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.hvplot.image(x='x', y='y', rasterize=True,\n",
    "                data_aspect=1,\n",
    "                cmap='reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03334c-2ba3-4065-97a2-b2f8a3b67001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmm... why are the y-axis units negative? \n",
    "# https://www.usgs.gov/faqs/why-do-landsat-scenes-southern-hemisphere-display-negative-utm-values?qt-news_science_products=0#qt-news_science_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b20391-a8e3-4d0a-ac26-f829a3c1f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stackstac is a powerful library for converting many STAC Items to an xarray dataarray\n",
    "\n",
    "daL8 = stackstac.stack(\n",
    "    [x.to_dict() for x in signed_items], assets=[\"SR_B4\", \"SR_B5\"],\n",
    ")\n",
    "daL8.name = 'landsat-8-c2-l2'\n",
    "daL8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae47cb-bdd1-401c-921f-b01dbcade90b",
   "metadata": {},
   "source": [
    "Don't be scared by the huge *total* size of this dataset! That represents the uncompressed size of *all* 2D rasters, but often we just want a subset, and we can use Xarray's API + dask to retrieve a subset efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23644217-9980-4cf1-a615-631c5bee985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, let's use a small range of latitude and longitude\n",
    "transform = pyproj.Proj(da.rio.crs)\n",
    "left, top = transform(-69.8, -31.5)\n",
    "right, bottom = transform(-69.3, -32)\n",
    "print(left,right,bottom,top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177682d3-09de-49b5-920b-11ac81df0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = daL8.sel(x=slice(left,right), y=slice(top,bottom)) # note top,bottom b/c y coords stored in *decreasing* order\n",
    "DS = subset.to_dataset(dim='band')\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1572830-148b-41fa-a73d-5b2a20d88e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute NDVI \n",
    "NDVI = (DS.SR_B5 - DS.SR_B4) / (DS.SR_B5 + DS.SR_B4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15302586-5ac3-4b32-91b1-c3c0f503a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot interactive visualization\n",
    "result = NDVI.persist() # pull into cluster memory, watch the task graph to see work happening\n",
    "result.hvplot.image(x='x',y='y',\n",
    "                    rasterize=True, \n",
    "                    data_aspect=1, frame_width=400,\n",
    "                    cmap='BrBG', clim=(-1,1), \n",
    "                    widget_location='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d5bdb-f885-45cf-a4b3-f20fa569c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = result.sel(x=slice(4.52e5,4.56e5), y=slice(-3.500e6, -3.501e6)).mean(dim=['x','y']).compute()\n",
    "time_series.name = 'NDVI'\n",
    "time_series.hvplot.scatter(x='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85225fe6-e14a-4b7f-8d31-a8b03e801e5e",
   "metadata": {},
   "source": [
    "**Keep going!**\n",
    "- examine some quality bands and use them to mask your results\n",
    "- look at different indices such as EVI https://www.usgs.gov/core-science-systems/nli/landsat/landsat-enhanced-vegetation-index?qt-science_support_page_related_con=0#qt-science_support_page_related_con or NDSI\n",
    "- mask or look a NDVI statistics versus elevation with NASADEM\n",
    "- use xarray advanced interpolation to extract values at many points\n",
    "- compute mean NDVI and save resulting 2D geotiff\n",
    "- bring in an open dataset in a different cloud data center like Copernicus DEM https://registry.opendata.aws/copernicus-dem/ \n",
    "- dig into the dask diagnotics and experiment with chunk settings for your data and workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b5e71-2bc3-4dd7-8f7e-07cdbc4a09a2",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "There are lots of great tutorials out there to continue to explore.\n",
    "\n",
    "- The Official Documentation for each Library\n",
    "- [Pangeo Gallery Tutorial](https://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/)\n",
    "- [Earthlab Data Science Course](https://www.earthdatascience.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
